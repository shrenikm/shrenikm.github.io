<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://shrenikm.com/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>shrenik95@gmail.com</managingEditor>
    <webMaster>shrenik95@gmail.com</webMaster>
    <lastBuildDate>Sun, 29 Aug 2021 10:20:38 -0700</lastBuildDate><atom:link href="https://shrenikm.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dimension of a proper subspace</title>
      <link>https://shrenikm.com/posts/2021-08-29-dimension-of-a-proper-subspace/</link>
      <pubDate>Sun, 29 Aug 2021 10:20:38 -0700</pubDate>
      <author>shrenik95@gmail.com</author>
      <guid>https://shrenikm.com/posts/2021-08-29-dimension-of-a-proper-subspace/</guid>
      <description>Let $V$ be a finite dimensional vector space over the field $F$, with $\dim{V} = n$.
We also define the ordered basis for $V$ to be $B = \set{\beta_1, \ldots, \beta_n}$.
Consider a subspace $U$ of the vector space $V \subseteq U$. Naturally, we have $\dim{U} \leq \dim{V}$.
This is because for any $\alpha \in U$, we have $\alpha \in V$, which can then be expressed as a linear combination of the basis vectors in $B$.</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://shrenikm.com/pages/projects/</link>
      <pubDate>Sat, 28 Aug 2021 16:53:20 -0700</pubDate>
      <author>shrenik95@gmail.com</author>
      <guid>https://shrenikm.com/pages/projects/</guid>
      <description>GPV Generation of policy variations in deep reinforcement learning 

The project aims to enable reinforcement learning agents to obtain probabilistically consistant policy variations from a learned deterministic policy. A Variational Autoencoder (VAE) is used to obtain these variations. The goal is to enable agents to execute actions that are different (and sub-optimal) from the trained policy, while still being able to achieve the desired result.
This is useful in cases where we need to generate behavior that is not a part of the trained policy, while also making sure that the system does not fail at the given task.</description>
    </item>
    
    <item>
      <title>Shrenik Muralidhar</title>
      <link>https://shrenikm.com/pages/about/</link>
      <pubDate>Sat, 28 Aug 2021 16:52:40 -0700</pubDate>
      <author>shrenik95@gmail.com</author>
      <guid>https://shrenikm.com/pages/about/</guid>
      <description>I&amp;rsquo;m a roboticist, currently working as a motion planning research engineer at Brain Corporation.
I have a Master&amp;rsquo;s degree in Robotics from the University of Pennsylvania. While at University, I was a part of the DAIR Lab, working on optimal control for bipedal robots.
My interests lie in enabling robots to function autonomously in the real world, using planning, optimization and learning based approaches.
Brief descriptions of some of my projects can be found here.</description>
    </item>
    
  </channel>
</rss>
